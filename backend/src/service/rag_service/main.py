from __future__ import annotations
import json
import os
import re
import fnmatch
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional
from src.service.rag_service.models import RawDocument, AllDocument
from src.service.rag_service.utils import Logger

SUPPORTED_TEXT_EXTENSIONS = {".txt", ".md", ".json", ".csv", ".log"}
logger = Logger.get_logger(__name__)


class CaseDocumentLoader:
    """Locate and read case resources generated by the extraction pipeline."""

    def __init__(
        self,
        resource_root: Path | None = None,
        allowed_extensions: Iterable[str] | None = None,
    ) -> None:
        if "__file__" in globals():
            base_dir = Path(__file__).resolve().parent.parent.parent.parent
        else:
            base_dir = Path(os.getcwd()).resolve().parent.parent.parent.parent
        path = f"{base_dir}/{resource_root}"
        default_root = f"{base_dir}/resources"
        self.resource_root = path if resource_root else default_root
        self.allowed_extensions = {
            ext.lower() if ext.startswith(".") else f".{ext.lower()}"
            for ext in (allowed_extensions or SUPPORTED_TEXT_EXTENSIONS)
        }
        if not Path(self.resource_root).exists():
            logger.error("Path not found")
            return

    def resolve_case_dir(self, case_id: str) -> Path:
        case_id = case_id.strip()
        if not isinstance(self.resource_root, Path):
            self.resource_root = Path(self.resource_root).expanduser().resolve()

        case_dir = (self.resource_root / case_id).resolve()

        if not case_dir.exists() or not case_dir.is_dir():
            # Try a fallback search anywhere under resource_root
            matches = list(self.resource_root.rglob(case_id))
            if matches:
                print("Found by rglob:", matches[0])
                return matches[0]
            raise FileNotFoundError(
                f"Case directory not found for '{case_id}'. "
                f"Tried: {case_dir}. resource_root is: {self.resource_root}"
            )
        return case_dir

    def iter_output_files(self, case_id: str) -> List[tuple[Path, str]]:
        case_dir = self.resolve_case_dir(case_id)
        files: List[tuple[Path, str]] = []
        for document_dir in sorted(p for p in case_dir.iterdir() if p.is_dir()):
            output_dir = document_dir / "output"
            if not output_dir.exists():
                continue
            for file_path in sorted(output_dir.rglob("*")):
                if (
                    file_path.is_file()
                    and file_path.suffix.lower() in self.allowed_extensions
                ):
                    files.append((file_path, document_dir.name))

        final_dir = case_dir / "final_output"
        if final_dir.exists():
            for file_path in sorted(final_dir.rglob("*")):
                if (
                    file_path.is_file()
                    and file_path.suffix.lower() in self.allowed_extensions
                ):
                    files.append((file_path, "final_output"))

        if not files:
            raise FileNotFoundError(
                f"No supported text documents found under 'output' for case '{case_id}'."
            )
        return files

    def load_case_documents(self, case_id: str) -> List[RawDocument]:
        documents: List[RawDocument] = []
        for file_path, document_type in self.iter_output_files(case_id):
            text, _ = self._read_text(file_path)
            if not text.strip():
                continue
            documents.append(
                RawDocument(
                    case_id=case_id,
                    document_type=document_type,
                    path=file_path,
                    text=text,
                )
            )
        if not documents:
            raise ValueError(f"Case '{case_id}' contains no readable documents.")
        return documents

    def _read_text(self, path: Path) -> tuple[str, str]:
        suffix = path.suffix.lower()
        try:
            if suffix == ".json":
                data = json.loads(path.read_text(encoding="utf-8"))
                return json.dumps(data, indent=2), suffix
            return path.read_text(encoding="utf-8", errors="ignore"), suffix
        except UnicodeDecodeError:
            return path.read_text(encoding="latin-1", errors="ignore"), suffix

    def _normalize_stmt_name(self, filename: str) -> str:
        """
        Map pipeline-prefixed names to canonical keys.
        'bank-statements.json'          -> 'statements.json'
        'bank-statements_kpis.json'     -> 'statements_kpis.json'
        Also accepts singular 'bank-statement*' variants.
        """
        name = filename.lower()
        # Strip leading 'bank-statements' or 'bank_statement'
        m = re.match(r"^(bank[-_]?statements?)[-_]?(.+)$", name)
        if m:
            name = m.group(2)  # part after the prefix
        return name

    def select_named_documents(
        self,
        docs: Iterable["RawDocument"],
        wanted: Iterable[str] = ("statements.json", "statements_kpis.json"),
        document_type: str = "bank-statements",
        patterns: Iterable[str] | None = None,
    ) -> List["RawDocument"]:
        """
        Filter a list of RawDocument (already loaded) by document_type and canonical names.
        No path reading; it uses the provided RawDocument.text.

        - wanted: exact canonical names after normalization (e.g., 'statements.json')
        - patterns: optional glob patterns (e.g., '*.json') applied to normalized names
        """
        wanted_set = {w.lower() for w in wanted}
        pat_list = list(patterns or [])

        hits: List["RawDocument"] = []
        for d in docs:
            if d.document_type != document_type:
                continue
            fname = Path(d.path).name.lower()
            norm = self._normalize_stmt_name(fname)

            ok = (norm in wanted_set) or any(fnmatch.fnmatch(norm, p) for p in pat_list)
            if ok:
                hits.append(d)

        # Optional: keep a stable order (by normalized name then path)
        hits.sort(
            key=lambda x: (self._normalize_stmt_name(Path(x.path).name), str(x.path))
        )
        return hits

    def load_case_all_documents(self, case_id: str) -> AllDocument:
        docs: Dict[str, Dict[str, Any]] = {}
        case_path: Optional[Path] = None
        kpis = md = ""

        for file_path, document_type in self.iter_output_files(case_id):
            text, file_type = self._read_text(file_path)
            if not text.strip():
                continue

            # normalize key like "bank-statements" -> "bank_statement"
            key = document_type.replace("-", "_")
            if file_type == ".json":
                kpis = text
            elif file_type == ".md":
                md = text
            docs[key] = {"kpis": kpis or {}, "text": text, "md": md or ""}

            if case_path is None:
                # keep a representative path (e.g., the first file's parent)
                case_path = file_path.parent

        if not docs:
            raise ValueError(f"Case '{case_id}' contains no readable documents.")

        return AllDocument(case_id=case_id, docs=docs, path=case_path)
