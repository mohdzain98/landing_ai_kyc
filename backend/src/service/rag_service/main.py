"""Document loading utilities used throughout the RAG service."""

from __future__ import annotations
import json
import os
import re
import fnmatch
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional
from langchain_aws import ChatBedrock
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from src.service.rag_service.models import RawDocument, AllDocument
from src.service.rag_service.utils import Logger, Config

SUPPORTED_TEXT_EXTENSIONS = {".txt", ".md", ".json", ".csv", ".log"}
logger = Logger.get_logger(__name__)


class CaseDocumentLoader:
    """Locate and read case resources generated by the extraction pipeline.

    Handles file-system lookups and text loading for every case."""

    def __init__(
        self,
        resource_root: Path | None = None,
        allowed_extensions: Iterable[str] | None = None,
    ) -> None:
        """Prepare search roots and extension filters for loading."""
        if "__file__" in globals():
            base_dir = Path(__file__).resolve().parent.parent.parent.parent
        else:
            base_dir = Path(os.getcwd()).resolve().parent.parent.parent.parent
        path = f"{base_dir}/{resource_root}"
        default_root = f"{base_dir}/resources"
        self.resource_root = path if resource_root else default_root
        self.allowed_extensions = {
            ext.lower() if ext.startswith(".") else f".{ext.lower()}"
            for ext in (allowed_extensions or SUPPORTED_TEXT_EXTENSIONS)
        }
        if not Path(self.resource_root).exists():
            logger.error("Path not found")
            return

    def resolve_case_dir(self, case_id: str) -> Path:
        """Return the concrete directory for a case, searching parents if needed."""
        case_id = case_id.strip()
        if not isinstance(self.resource_root, Path):
            self.resource_root = Path(self.resource_root).expanduser().resolve()

        case_dir = (self.resource_root / case_id).resolve()

        if not case_dir.exists() or not case_dir.is_dir():
            # Try a fallback search anywhere under resource_root
            matches = list(self.resource_root.rglob(case_id))
            if matches:
                return matches[0]
            raise FileNotFoundError(
                f"Case directory not found for '{case_id}'. "
                f"Tried: {case_dir}. resource_root is: {self.resource_root}"
            )
        return case_dir

    def iter_output_files(self, case_id: str) -> List[tuple[Path, str]]:
        """Yield every readable file under the case's output folders."""
        case_dir = self.resolve_case_dir(case_id)
        files: List[tuple[Path, str]] = []
        for document_dir in sorted(p for p in case_dir.iterdir() if p.is_dir()):
            output_dir = document_dir / "output"
            if not output_dir.exists():
                continue
            for file_path in sorted(output_dir.rglob("*")):
                if (
                    file_path.is_file()
                    and file_path.suffix.lower() in self.allowed_extensions
                ):
                    files.append((file_path, document_dir.name))

        final_dir = case_dir / "final_output"
        if final_dir.exists():
            for file_path in sorted(final_dir.rglob("*")):
                if (
                    file_path.is_file()
                    and file_path.suffix.lower() in self.allowed_extensions
                ):
                    files.append((file_path, "final_output"))

        if not files:
            raise FileNotFoundError(
                f"No supported text documents found under 'output' for case '{case_id}'."
            )
        return files

    def load_case_documents(self, case_id: str) -> List[RawDocument]:
        """Map each discovered file into a RawDocument instance."""
        documents: List[RawDocument] = []
        for file_path, document_type in self.iter_output_files(case_id):
            text, _ = self._read_text(file_path)
            if not text.strip():
                continue
            documents.append(
                RawDocument(
                    case_id=case_id,
                    document_type=document_type,
                    path=file_path,
                    text=text,
                )
            )
        if not documents:
            raise ValueError(f"Case '{case_id}' contains no readable documents.")
        return documents

    def _read_text(self, path: Path) -> tuple[str, str]:
        """Read a file as text, returning the content plus suffix."""
        suffix = path.suffix.lower()
        try:
            if suffix == ".json":
                data = json.loads(path.read_text(encoding="utf-8"))
                return json.dumps(data, indent=2), suffix
            return path.read_text(encoding="utf-8", errors="ignore"), suffix
        except UnicodeDecodeError:
            return path.read_text(encoding="latin-1", errors="ignore"), suffix

    def _normalize_stmt_name(self, filename: str) -> str:
        """Map pipeline-prefixed statement names to canonical keys.

        Supports plural/singular variants and shorthand patterns."""
        name = filename.lower()
        # Strip leading 'bank-statements' or 'bank_statement'
        m = re.match(r"^(bank[-_]?statements?)[-_]?(.+)$", name)
        if m:
            name = m.group(2)  # part after the prefix
        return name

    def select_named_documents(
        self,
        docs: Iterable["RawDocument"],
        wanted: Iterable[str] = ("statements.json", "statements_kpis.json"),
        document_type: str = "bank-statements",
        patterns: Iterable[str] | None = None,
    ) -> List["RawDocument"]:
        """Filter RawDocuments by document_type plus canonical filename hints.

        Supports both explicit matches and optional glob patterns."""
        wanted_set = {w.lower() for w in wanted}
        pat_list = list(patterns or [])

        hits: List["RawDocument"] = []
        for d in docs:
            if d.document_type != document_type:
                continue
            fname = Path(d.path).name.lower()
            norm = self._normalize_stmt_name(fname)

            ok = (norm in wanted_set) or any(fnmatch.fnmatch(norm, p) for p in pat_list)
            if ok:
                hits.append(d)

        # Optional: keep a stable order (by normalized name then path)
        hits.sort(
            key=lambda x: (self._normalize_stmt_name(Path(x.path).name), str(x.path))
        )
        return hits

    def load_case_all_documents(self, case_id: str) -> AllDocument:
        """Load every readable artifact and return an AllDocument bundle."""
        docs: Dict[str, Dict[str, Any]] = {}
        case_path: Optional[Path] = None
        kpis = md = ""

        for file_path, document_type in self.iter_output_files(case_id):
            text, file_type = self._read_text(file_path)
            if not text.strip():
                continue

            # normalize key like "bank-statements" -> "bank_statement"
            key = document_type.replace("-", "_")
            if file_type == ".json":
                kpis = text
            elif file_type == ".md":
                md = text
            docs[key] = {"kpis": kpis or {}, "text": text, "md": md or ""}

            if case_path is None:
                # keep a representative path (e.g., the first file's parent)
                case_path = file_path.parent

        if not docs:
            raise ValueError(f"Case '{case_id}' contains no readable documents.")

        return AllDocument(case_id=case_id, docs=docs, path=case_path)

    def load_kpi_definitions(self, case_id: str) -> Optional[RawDocument]:
        """Load the shared KPI definition reference as a RawDocument.

        Returns None when the resource is missing or empty."""
        service_dir = Path(__file__).resolve().parent
        candidate_paths = [
            service_dir / "kpi_definition.json",
            Path(self.resource_root).parent
            / "src/service/rag_service/kpi_definitions.json",
        ]

        file_path = next((p for p in candidate_paths if p.exists()), None)
        if file_path is None:
            logger.warning("KPI definitions file not found; skipping reference load.")
            return None

        try:
            text = file_path.read_text(encoding="utf-8")
            logger.info("Loaded KPI definitions from %s", file_path)
        except Exception as exc:
            logger.error("Failed to read KPI definitions (%s)", exc)
            return None

        cleaned = text.strip()
        if not cleaned:
            return None

        return RawDocument(
            case_id=case_id,
            document_type="kpi_definitions",
            path=file_path,
            text=cleaned,
        )


class KPIReferenceLoader:
    def __init__(self):
        self.FINAL_RE = re.compile(
            r"\b(final(\s*decision)?|decision|decline|approve|outcome)\b", re.I
        )
        self.KPI_RE = re.compile(
            r"\b(kpi|kpis|key\s*performance\s*indicator[s]?|metric|definition)\b", re.I
        )
        self.GREETING_RE = re.compile(
            r"^\s*(hi+|hello|hey|hiya|yo|thanks|thank\s*you|thank\s*u)\b.*$", re.I
        )
        self.config = Config()

    def heuristic_intents(self, q: str) -> Dict[str, bool]:
        logger.info("Using heuristic intent detection for question.")
        return {
            "decision_intent": bool(self.FINAL_RE.search(q or "")),
            "kpi_intent": bool(self.KPI_RE.search(q or "")),
        }

    def llm_intents(self, q: str) -> Dict[str, bool]:
        logger.info("Using LLM intent detection for question.")
        intent_system = """You classify user questions for retrieval augmentation.
        Return ONLY JSON with two booleans: {{ "decision_intent": true, "kpi_intent": false }}
        Definitions:
        - decision_intent: user asks about final decision, approval/decline, reason, outcome.
        - kpi_intent: user asks about KPIs or KPI definitions/meaning/metrics.
        No extra text.
        """

        intent_user = """User question:
        {question}

        Return JSON only.
        """

        intent_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", intent_system),
                ("user", intent_user),
            ]
        )

        llm_small = ChatBedrock(
            model_id="amazon.nova-micro-v1:0",
            region="us-east-1",
            aws_access_key_id=self.config.aws_access_key,
            aws_secret_access_key=self.config.aws_secret_key,
            temperature=0,
        )
        intent_chain = intent_prompt | llm_small | StrOutputParser()
        raw = intent_chain.invoke({"question": q})
        try:
            data = json.loads(raw)
            return {
                "decision_intent": bool(data.get("decision_intent", False)),
                "kpi_intent": bool(data.get("kpi_intent", False)),
            }
        except Exception:
            return self.heuristic_intents(q)

    def detect_intents(self, question: str) -> Dict[str, bool]:
        if self.GREETING_RE.match(question or ""):
            return {"decision_intent": False, "kpi_intent": False}
        h = self.heuristic_intents(question)
        if h["decision_intent"] or h["kpi_intent"]:
            return h
        return self.llm_intents(question)
